{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"bci/","title":"Bci","text":"<p>In progress</p>"},{"location":"cal/","title":"Cal","text":"<p>In progress</p>"},{"location":"edge/","title":"EDGE-AI","text":""},{"location":"edge/#introduction","title":"Introduction","text":"<p>Recent strides in the efficacy of AI, the adoption of IoT devices and the power of edge computing have come together to unlock the power of edge AI. This has opened new opportunities for edge AI that were previously unimaginable \u2014 from helping radiologists identify pathologies in the hospital, to driving cars down the freeway, to helping us pollinate plants. Countless analysts and businesses are talking about and implementing edge computing, which traces its origins to the 1990s, when content delivery networks were created to serve web and video content from edge servers deployed close to users. Today, almost every business has job functions that can benefit from the adoption of edge AI. In fact, edge applications are driving the next wave of AI computing in ways that improve our lives at home, at work, in school and in transit. Learn more about what edge AI is, its benefits and how it works, examples of edge AI use cases, and the relationship between edge computing and cloud computing.</p>"},{"location":"edge/#what-is-edge-ai","title":"What Is Edge AI?","text":"<p>Edge AI is the deployment of AI applications in devices throughout the physical world. It\u2019s called \u201cedge AI\u201d because the AI computation is done near the user at the edge of the network, close to where the data is located, rather than centrally in a cloud computing facility or private data center. Since the internet has global reach, the edge of the network can connote any location. It can be a retail store, factory, hospital or devices all around us, like traffic lights, autonomous machines and phones.</p>"},{"location":"edge/#edge-ai-why-now","title":"Edge AI: Why Now?","text":"<p>Organizations from every industry are looking to increase automation to improve processes, efficiency and safety.</p> <p>To help them, computer programs need to recognize patterns and execute tasks repeatedly and safely. But the world is unstructured and the range of tasks that humans perform covers infinite circumstances that are impossible to fully describe in programs and rules.</p> <p>Advances in edge AI have opened opportunities for machines and devices, wherever they may be, to operate with the \u201cintelligence\u201d of human cognition. AI-enabled smart applications learn to perform similar tasks under different circumstances, much like real life.</p>"},{"location":"edge/#the-efficacy-of-deploying-ai-models-at-the-edge-arises","title":"The efficacy of deploying AI models at the edge arises","text":"<p>The efficacy of deploying AI models at the edge arises from three recent innovations.</p> <ul> <li> <p>Maturation of neural networks : </p> </li> </ul> <p>Neural networks and related AI infrastructure have finally developed to the point of allowing for generalized machine learning. Organizations are learning how to successfully train AI models and deploy them in production at the edge.</p> <ul> <li> <p>Advances in compute infrastructure : </p> </li> </ul> <p>Powerful distributed computational power is required to run AI at the edge. Recent advances in highly parallel GPUs have been adapted to execute neural networks. </p> <ul> <li> <p>Adoption of IoT devices: </p> </li> </ul> <p>The widespread adoption of the Internet of Things has fueled the explosion of big data. With the sudden ability to collect data in every aspect of a business \u2014 from industrial sensors, smart cameras, robots and more \u2014 we now have the data and devices necessary to deploy AI models at the edge. Moreover, 5G is providing IoT a boost with faster, more stable and secure connectivity. </p>"},{"location":"edge/#why-deploy-ai-at-the-edge","title":"Why Deploy AI at the Edge?","text":"<p>Since AI algorithms are capable of understanding language, sights, sounds, smells, temperature, faces and other analog forms of unstructured information, they\u2019re particularly useful in places occupied by end users with real-world problems. These AI applications would be impractical or even impossible to deploy in a centralized cloud or enterprise data center due to issues related to latency, bandwidth and privacy.</p>"},{"location":"edge/#what-are-the-benefits-of-edge-ai","title":"What Are the Benefits of Edge AI?","text":"<p>The benefits of edge AI include:</p> <ul> <li> <p>Intelligence: </p> </li> </ul> <p>AI applications are more powerful and flexible than conventional applications that can respond only to inputs that the programmer had anticipated. In contrast, an AI neural network is not trained how to answer a specific question, but rather how to answer a particular type of question, even if the question itself is new. Without AI, applications couldn\u2019t possibly process infinitely diverse inputs like texts, spoken words or video. </p> <ul> <li> <p>Real-time insight: </p> </li> </ul> <p>Since edge technology analyzes data locally rather than in a faraway cloud delayed by long-distance communications, it responds to users\u2019 needs in real time. </p> <ul> <li> <p>Reduced cost : </p> </li> </ul> <p>By bringing processing power closer to the edge, applications need less internet bandwidth, greatly reducing networking costs. </p> <ul> <li> <p>Increased privacy: </p> </li> </ul> <p>AI can analyze real-world information without ever exposing it to a human being, greatly increasing privacy for anyone whose appearance, voice, medical image or any other personal information needs to be analyzed. Edge AI further enhances privacy by containing that data locally, uploading only the analysis and insights to the cloud. Even if some of the data is uploaded for training purposes, it can be anonymized to protect user identities. By preserving privacy, edge AI simplifies the challenges associated with data regulatory compliance. </p> <ul> <li> <p>High availability: </p> </li> </ul> <p>Decentralization and offline capabilities make edge AI more robust since internet access is not required for processing data. This results in higher availability and reliability for mission-critical, production-grade AI applications. </p> <ul> <li> <p>Persistent improvement </p> </li> </ul> <p>AI models grow increasingly accurate as they train on more data. When an edge AI application confronts data that it cannot accurately or confidently process, it typically uploads it so that the AI can retrain and learn from it. So the longer a model is in production at the edge, the more accurate the model will be. </p>"},{"location":"edge/#how-does-edge-ai-technology-work","title":"How Does Edge AI Technology Work?","text":"<p>For machines to see, perform object detection, drive cars,understand speech, speak, walk or otherwise emulate human skills, they need to functionally replicate human intelligence.AI employs a data structure called a deep neural network to replicate human cognition. These DNNs are trained to answer specific types of questions by being shown many examples of that type of question along with correct answers.This training process, known as \u201cdeep learning,\u201d often runs in a data center or the cloud due to the vast amount of data required to train an accurate model, and the need for data scientists to collaborate on configuring the model. After training, the model graduates to become an \u201cinference engine\u201d that can answer real-world questions.In edge AI deployments, the inference engine runs on some kind of computer or device in far-flung locations such as factories, hospitals, cars, satellites and homes. When the AI stumbles on a problem, the troublesome data is commonly uploaded to the cloud for further training of the original AI model, which at some point replaces the inference engine at the edge. This feedback loop plays a significant role in boosting model performance; once edge AI models are deployed, they only get smarter and smarter.</p> <p>L'IA est la force technologique la plus puissante de notre \u00e9poque. Nous</p>"},{"location":"edge/#what-are-examples-of-edge-ai-use-cases","title":"What Are Examples of Edge AI Use Cases?","text":"<p>AI is the most powerful technology force of our time. We\u2019re now at a time where AI is revolutionizing the world\u2019s largest industries. Across manufacturing, healthcare, financial services, transportation, energy and more, edge AI is driving new business outcomes in every sector, including:</p> <ul> <li> <p>Intelligent forecasting in energy : </p> </li> </ul> <p>For critical industries such as energy, in which discontinuous supply can threaten the health and welfare of the general population, intelligent forecasting is key. Edge AI models help to combine historical data, weather patterns, grid health and other information to create complex simulations that inform more efficient generation, distribution and management of energy resources to customers. </p> <ul> <li> <p>Predictive maintenance in manufacturing: </p> </li> </ul> <p>Sensor data can be used to detect anomalies early and predict when a machine will fail. Sensors on equipment scan for flaws and alert management if a machine needs a repair so the issue can be addressed early, avoiding costly downtime. </p> <ul> <li> <p>AI-powered instruments in healthcare: </p> </li> </ul> <p>Modern medical instruments at the edge are becoming AI-enabled with devices that use ultra-low-latency streaming of surgical video to allow for minimally invasive surgeries and insights on demand. </p> <ul> <li> <p>Smart virtual assistants in retail: </p> </li> </ul> <p>Retailers are looking to improve the digital customer experience by introducing voice ordering to replace text-based searches with voice commands. With voice ordering, shoppers can easily search for items, ask for product information and place online orders using smart speakers or other intelligent mobile devices.</p>"},{"location":"edge/#what-role-does-cloud-computing-play-in-edge-computing","title":"What Role Does Cloud Computing Play in Edge Computing?","text":"<p>AI applications can run in a data center like those in public clouds, or out in the field at the network\u2019s edge, near the user. Cloud computing and edge computing each offer benefits that can be combined when deploying edge AI.</p> <p>The cloud offers benefits related to infrastructure cost, scalability, high utilization, resilience from server failure, and collaboration. Edge computing offers faster response times, lower bandwidth costs and resilience from network failure.</p> <p>There are several ways in which cloud computing can support an edge AI deployment:</p> <ul> <li> <p>The cloud can run the model during its training period. </p> </li> <li> <p>The cloud continues to run the model as it is retrained with data that comes from the edge. </p> </li> <li> <p>The cloud can run AI inference engines that supplement the models in the field when high compute power is more important than response time. For example, a voice assistant might respond to its name, but send complex requests back to the cloud for parsing. </p> </li> <li> <p>The cloud serves up the latest versions of the AI model and application. </p> </li> <li> <p>The same edge AI often runs across a fleet of devices in the field with software in the cloud. </p> </li> </ul>"},{"location":"edge/#the-future-of-edge-ai","title":"The Future of Edge AI","text":"<p>Thanks to the commercial maturation of neural networks, proliferation of IoT devices, advances in parallel computation and 5G, there is now robust infrastructure for generalized machine learning. This is allowing enterprises to capitalize on the colossal opportunity to bring AI into their places of business and act upon real-time insights, all while decreasing costs and increasing privacy.</p> <p>We are only in the early innings of edge AI, and still the possible applications seem endless.</p>"},{"location":"","title":"Introduction","text":""},{"location":"#what-is-artificial-intelligence-ai","title":"What is Artificial Intelligence (AI)?","text":"<p>Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing, speech recognition, and computer vision.</p>"},{"location":"#how-does-ai-work","title":"How does AI work?","text":"<p>As the hype around AI has accelerated, vendors have scrambled to promote how their products and services use AI. Often what they call AI is just a component of AI, like machine learning. AI requires a foundation of specialized hardware and software for writing and training machine learning algorithms. No programming language is synonymous with AI, but a few, including Python, R, and Java, are popular.</p> <p>In general, AI systems work by ingesting large amounts of labeled training data, analyzing the data for correlations and patterns, and using those patterns to make predictions about future states. In this way, a chatbot that receives examples of text chats can learn to produce realistic interactions with people, or an image recognition tool can learn to identify and describe objects in images by examining millions of examples.</p> <p>AI programming focuses on three cognitive skills: learning, reasoning, and self-correction.</p> <ul> <li>Learning process:</li> </ul> <p>This aspect of AI programming focuses on acquiring data and creating rules on how to turn data into actionable insights. Rules, called algorithms, provide computing devices with step-by-step instructions on how to perform a specific task.</p> <ul> <li>Reasoning process:</li> </ul> <p>This aspect of AI programming focuses on choosing the right algorithm to achieve the desired result.</p> <ul> <li>Self-correction process:</li> </ul> <p>This aspect of AI programming is designed to continually refine the algorithms and ensure that they provide the most accurate results possible.</p>"},{"location":"#what-are-the-advantages-and-disadvantages-of-artificial-intelligence","title":"What are the advantages and disadvantages of artificial intelligence?","text":"<p>Artificial neural networks and deep learning AI technologies are evolving rapidly, primarily because AI processes large amounts of data much faster and makes more accurate predictions than humanly possible.</p> <p>While the huge volume of data created daily would bury a human researcher, AI applications that use machine learning can take that data and quickly turn it into actionable insights. As of this writing, the main drawback of using AI is that it is expensive to process the large amounts of data required for AI programming.</p> <ul> <li>advantages :</li> </ul> <p>Good for detail-oriented work;</p> <p>Reduced time for data-intensive tasks;</p> <p>AI-powered virtual agents are always available.</p> <ul> <li>disadvantages :</li> </ul> <p>Dear;</p> <p>Requires deep technical expertise;</p> <p>Limited supply of skilled workers to create AI tools; Lack of ability to generalize from task to task.</p>"},{"location":"#strong-ai-vs-weak-ai","title":"Strong AI vs Weak AI","text":"<p>AI can be categorized as weak or strong.</p> <ul> <li>Weak AI:</li> </ul> <p>Also known as Narrow AI, is an AI system designed and trained to perform a specific task. Industrial robots and virtual personal assistants, such as Apple's Siri, use weak AI.</p> <ul> <li>Strong AI :</li> </ul> <p>Also known as artificial general intelligence (AGI), describes programming capable of replicating the cognitive abilities of the human brain. When faced with an unknown task, a powerful AI system can use fuzzy logic to apply knowledge from one domain to another and find a solution on its own. In theory, a strong AI program should be able to pass both a Turing test and the Chinese hall test.</p>"},{"location":"#what-are-the-4-types-of-artificial-intelligence","title":"What are the 4 types of artificial intelligence?","text":"<p>Arend Hintze, assistant professor of integrative biology and computer science and engineering at Michigan State University, explained in a 2016 article that AI can be categorized into four types, starting with task-specific intelligent systems widely used today and progressing towards sensitive systems. , which do not yet exist. The categories are as follows:</p> <ul> <li>Type 1: Reactive machines:</li> </ul> <p>These AI systems have no memory and are task specific. An example is Deep Blue, the IBM chess program that beat Garry Kasparov in the 1990s. Deep Blue can identify pieces on the board and make predictions, but since it has no memory, it cannot use past experiences to inform future ones.</p> <ul> <li>Type 2: Limited memory:</li> </ul> <p>These AI systems have memory, so they can use past experiences to inform future decisions. Some of the decision-making functions in self-driving cars are designed this way.</p> <ul> <li>Type 3: Theory of Mind:</li> </ul> <p>Theory of mind is a psychology term. Applied to AI, this means the system would have the social intelligence to understand emotions. This type of AI will be able to infer human intentions and predict behavior, a skill necessary for AI systems to become full members of human teams.</p> <ul> <li>Type 4: Self-awareness:</li> </ul> <p>In this category, AI systems have a sense of self, which gives them consciousness. Self-aware machines understand their own current state. This type of AI does not yet exist.</p>"},{"location":"#why-is-artificial-intelligence-important","title":"Why is artificial intelligence important?","text":"<p>AI is important because it can give companies insight into their operations that they may not have known before and because in some cases AI can perform tasks better than humans. Especially when it comes to repetitive and detailed tasks such as scanning a large number of legal documents to ensure that the relevant fields are filled in correctly, AI tools often perform the tasks quickly and with relatively few errors.</p> <p>This helped fuel an explosion in efficiency and opened the door to entirely new business opportunities for some large corporations. Before the current wave of AI, it would have been hard to imagine using computer software to connect riders to taxis, but today Uber has become one of the biggest companies in the world doing just that. It uses sophisticated machine learning algorithms to predict when people are likely to need rides in certain areas, helping to proactively get drivers on the road before they're needed.</p>"},{"location":"#what-are-examples-of-ai-technology-and-how-is-it-used-today-","title":"What are examples of AI technology and how is it used today ?","text":"<p>AI is embedded in a variety of different types of technologies.</p> <p>Here are six examples:</p> <p></p> <ul> <li>Automating:</li> </ul> <p>When combined with AI technologies, automation tools can increase the volume and types of tasks performed. One example is robotic process automation (RPA), a type of software that automates repetitive, rule-based data processing tasks traditionally performed by humans. When combined with machine learning and emerging AI tools, RPA can automate larger parts of business tasks, enabling tactical RPA bots to transmit AI insights and respond to process changes.</p> <p></p> <ul> <li>Machine Learning:</li> </ul> <p>It is the science of making a computer work without programming. Deep learning is a subset of machine learning which, in very simple terms, can be thought of as the automation of predictive analytics. There are three types of machine learning algorithms:</p> <p></p> <ul> <li>Supervised learning:</li> </ul> <p>Datasets are labeled so that patterns can be detected and used to label new datasets.</p> <p></p> <ul> <li>Unsupervised learning:</li> </ul> <p>Datasets are unlabeled and sorted based on similarities or differences.</p> <p></p> <ul> <li>Reinforcement learning:</li> </ul> <p>The datasets are not labeled but, after performing an action or several actions, the AI system receives feedback.</p> <p></p> <ul> <li>Artificial vision:</li> </ul> <p>This technology gives a machine the ability to see. Computer vision captures and analyzes visual information using a camera, analog-to-digital conversion, and digital signal processing. It is often compared to human sight, but artificial vision is not related to biology and can be programmed to see through walls, for example. It is used in a range of applications from signature identification to medical image analysis. Computer vision, which focuses on computer image processing, is often confused with machine vision.</p> <p></p> <ul> <li>Natural Language Processing (NLP):</li> </ul> <p>It is the processing of human language by a computer program. One of the oldest and best-known examples of NLP is spam detection, which examines the subject line and text of an email and decides whether it is spam. Current approaches to NLP are based on machine learning. NLP tasks include text translation, sentiment analysis, and voice recognition.</p> <p></p> <ul> <li>Robotics</li> </ul> <p>This field of engineering focuses on the design and manufacture of robots. Robots are often used to perform tasks that are difficult for humans to perform or perform consistently. For example, robots are used in assembly lines for automobile production or by NASA to move large objects through space. Researchers are also using machine learning to build robots that can interact in social contexts.</p> <p></p> <ul> <li>Autonomous cars</li> </ul> <p>Autonomous vehicles use a combination of computer vision, image recognition and deep learning to develop automated skills to steer a vehicle while staying in a given lane and avoiding unexpected obstacles, such as pedestrians.</p>"},{"location":"#what-are-the-applications-of-ai","title":"What are the applications of AI?","text":"<p>Artificial intelligence has made its way into a wide variety of markets.</p> <p>Here are nine examples:</p> <p></p> <ul> <li>AI in healthcare:</li> </ul> <p>The biggest bets are on improving patient outcomes and reducing costs. Companies are applying machine learning to make diagnoses better and faster than humans. One of the best-known healthcare technologies is IBM Watson. He understands natural language and can answer questions put to him. The system extracts patient data and other available data sources to formulate a hypothesis, which it then presents with a confidence scoring schema. Other AI applications include the use of online virtual healthcare assistants and chatbots to help healthcare patients and customers find medical information, schedule appointments, understand the billing process and to perform other administrative processes. An array of AI technologies are also used to predict, combat and understand pandemics such as COVID-19.</p> <p></p> <ul> <li>AI in business:</li> </ul> <p>Machine learning algorithms are integrated into analytics and customer relationship management (CRM) platforms to uncover insights on how to better serve customers. Chatbots have been integrated into websites to provide immediate service to customers. Workstation automation has also become a topic of discussion among academics and computer analysts.</p> <p></p> <ul> <li>AI in education:</li> </ul> <p>AI can automate grading, giving teachers more time. He can assess students and adapt to their needs, helping them work at their own pace. AI tutors can provide additional support to students, ensuring they stay on track. And it could change where and how students learn, maybe even replace some teachers.</p> <p></p> <ul> <li>AI in finance:</li> </ul> <p>AI in personal finance apps, such as Intuit Mint or TurboTax, is disrupting financial institutions. Such apps collect personal data and provide financial advice. Other programs, such as IBM Watson, have been applied to the home buying process. Today, artificial intelligence software drives much of the trading on Wall Street.</p> <p></p> <ul> <li>AI in law:</li> </ul> <p>The process of discovery - sifting through documents - in law is often overwhelming for humans. Using AI to help automate labor-intensive legal industry processes saves time and improves customer service. Law firms use machine learning to describe data and predict outcomes, computer vision to classify and extract information from documents, and natural language processing to interpret information requests.</p> <p></p> <ul> <li>AI in manufacturing:</li> </ul> <p>Manufacturing has been at the forefront of integrating robots into the workflow. For example, industrial robots that were once programmed to perform single tasks and separate from human workers are increasingly functioning as cobots: smaller, multi-tasking robots that collaborate with humans and take responsibility for more parts of work in warehouses, factories and other work spaces.</p> <p></p> <ul> <li>AI in banking:</li> </ul> <p>Banks are successfully using chatbots to inform their customers about services and offers and to manage transactions that do not require human intervention. AI virtual assistants are used to improve and reduce the cost of compliance with banking regulations. Banking organizations are also using AI to improve their lending decision-making, to set credit limits and identify investment opportunities.</p> <p></p> <ul> <li>AI in transport:</li> </ul> <p>Besides the fundamental role of AI in operating autonomous vehicles, AI technologies are being used in transportation to manage traffic, predict flight delays, and make shipping safer and more efficient.</p> <p></p> <ul> <li>Security:</li> </ul> <p>AI and machine learning top the list of buzzwords security vendors are using today to differentiate their offerings. These terms also represent truly viable technologies. Organizations use machine learning in security information and event management (SIEM) software and related fields to detect anomalies and identify suspicious activity that indicates threats. By analyzing data and using logic to identify similarities to known malicious code, AI can provide alerts to new and emerging attacks much earlier than human employees and previous technology iterations. Mature technology plays an important role in helping organizations fight cyberattacks.</p>"},{"location":"snn/","title":"SNNs","text":""},{"location":"snn/#spiking-neural-networkssnn","title":"SPIKING NEURAL NETWORKS(SNN)","text":"<p>Spiking neural networks ( SNNs ) are artificial neural networks that more closely mimic natural neural networks. In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential \u2013 an intrinsic quality of the neuron related to its membrane electrical charge \u2013 reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model  that fires at the moment of threshold crossing is also called a spiking neuron model.</p> <p>The most prominent spiking neuron model is the leaky integrate-and-fire model. In the integrate-and-fire model, the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher or lower, until the state eventually either decays or  - if the firing threshold is reached - the neuron fires. After firing the state variable is reset to a lower value.</p> <p>Various decoding methods exist for interpreting the outgoing spike train as a real-value number, relying on either the frequency of spikes (rate-code), the time-to-first-spike after stimulation, or the interval between spikes.</p>"},{"location":"snn/#underpinnings","title":"Underpinnings","text":"<p>Information in the brain is represented as action potentials (neuron spikes), which may be grouped into spike trains or even coordinated waves of brain activity. A fundamental question of neuroscience is to determine whether neurons communicate by a rate or temporal code. Temporal coding suggests that a single spiking neuron can replace hundreds of hidden units on a sigmoidal neural net .</p> <p>An SNN computes in the continuous rather than the discrete domain. The idea is that neurons may not test for activation in every iteration of propagation (as is the case in a typical multilayer perceptron network), but only when their membrane potentials reach a certain value. When a neuron is activated, it produces a signal that is passed to connected neurons, raising or lowering their membrane potential.</p> <p>In a spiking neural network, a neuron's current state is defined as its membrane potential (possibly modeled as a differential equation). An input pulse causes the membrane potential to rise for a period of time and then gradually decline. Encoding schemes have been constructed to interpret these pulse sequences as a number, taking into account both pulse frequency and pulse interval. A neural network model based on pulse generation time can be established. Using the exact time of pulse occurrence, a neural network can employ more information and offer better computing properties.</p> <p>The SNN approach produces a continuous output instead of the binary output of traditional ANNs. Pulse trains are not easily interpretable, hence the need for encoding schemes as above. However, a pulse train representation may be more suited for processing spatiotemporal data (or continual real-world sensory data classification). SNNs consider space by connecting neurons only to nearby neurons so that they process input blocks separately (similar to CNN using filters). They consider time by encoding information as pulse trains so as not to lose information in a binary encoding. This avoids the additional complexity of a recurrent neural network (RNN). It turns out that impulse neurons are more powerful computational units than traditional artificial neurons.</p> <p>SNNs are theoretically more powerful than second-generation networks  ; however, SNN training issues and hardware requirements limit their use. Although unsupervised biologically inspired learning methods are available such as Hebbian learning and STDP, no effective supervised training method is suitable for SNNs that can provide better performance than second-generation networks. Spike-based activation of SNNs is not differentiable thus making it hard to develop gradient descent based training methods to perform error backpropagation, though a few recent algorithms such as NormAD and multilayer NormAD have demonstrated good training performance through suitable approximation of the gradient of spike based activation.</p> <p>SNNs have much larger computational costs for simulating realistic neural models than traditional ANNs.</p> <p>Pulse-coupled neural networks (PCNN) are often confused with SNNs. A PCNN can be seen as a kind of SNN.</p> <p>Currently there are a few challenges when using SNNs that researchers  are actively working on. The first challenge concerns the nondifferentiability of the spiking nonlinearity. The expressions for both the forward- and backward-learning methods contain the derivative of the neural activation function which is non-differentiable because neuron's output is either 1 when it spikes, and 0 otherwise. This all-or-nothing behavior of the binary spiking nonlinearity stops gradients from \u201cflowing\u201d and makes LIF neurons unsuitable for gradient-based optimization. The second challenge concerns the implementation of the optimization algorithm itself. Standard BP can be expensive in terms of computation, memory, and communication and may be poorly suited to the constraints dictated by the hardware that implements it (e.g., a computer, brain, or neuromorphic device). Regarding the first challenge there are several approached in order to overcome it. A few of them are:</p> <ul> <li>resorting to entirely biologically inspired local learning rules for the hidden units</li> <li> <p>translating conventionally trained \u201crate-based\u201d NNs to SNNs</p> </li> <li> <p>smoothing the network model to be continuously differentiable</p> </li> <li>defining an SG (Surogate Gradient) as a continuous relaxation of the real gradients</li> </ul>"},{"location":"snn/#applications","title":"APPLICATIONS","text":"<p>SNNs can in principle apply to the same applications as traditional ANNs. In addition, SNNs can model the central nervous system of biological organisms, such as an insect seeking food without prior knowledge of the environment. Due to their relative realism, they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function, recordings of this circuit can be compared to the output of the corresponding SNN,evaluating the plausibility of the hypothesis. However, there is a lack of effective training mechanisms for SNNs, which can be inhibitory for some applications, including computer vision tasks.</p> <p>As of 2019 SNNs lag behind ANNs in terms of accuracy, but the gap is decreasing, and has vanished on some tasks.</p> <p>When using SNNs for image based data we need to convert static images into binary spike trains coding.</p> <p>Types of encodings:</p> <ul> <li> <p>Temporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity. </p> </li> <li> <p>Rate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity. </p> </li> <li> <p>Direct coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0. </p> </li> <li> <p>Phase coding encodes temporal information into spike patterns based on a global oscillator. </p> </li> <li> <p>Burst coding transmits the burst of spikes in a small-time duration, increasing the reliability of synaptic communication between neurons. </p> </li> </ul>"}]}